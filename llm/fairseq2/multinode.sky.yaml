# Fairseq2: Multi-node Fine-tuning on SkyPilot
#
# Fine-tune a Llama model across multiple nodes using fairseq2 with FSDP.
# Defaults to Llama 3.2 1B on 2 nodes.
#
# Usage:
#   sky launch -c fairseq2-multi multinode.sky.yaml --secret HF_TOKEN
#
# Use a larger model:
#   sky launch -c fairseq2-multi multinode.sky.yaml --secret HF_TOKEN \
#     --gpus H100:8 --env MODEL=llama3_1_70b

envs:
  MODEL: llama3_2_1b
  MAX_NUM_STEPS: 2000
  # Max sequence length (controls memory usage; reduce if OOM).
  # max_num_tokens is set to 2x this value, following fairseq2 defaults.
  MAX_SEQ_LEN: 2048
  # Change this to your own checkpoint bucket
  CHECKPOINT_BUCKET_NAME: sky-fairseq2-checkpoints

secrets:
  HF_TOKEN: null  # Pass with `--secret HF_TOKEN` in CLI

workdir:
  url: https://github.com/facebookresearch/fairseq2.git
  ref: main

resources:
  accelerators: L40S

num_nodes: 2

file_mounts:
  # Shared cloud storage for checkpoints.
  # NOTE: If you're on a Slurm cluster with NFS, you may remove this mount and
  # point the checkpoints symlink to a shared NFS path instead.
  /output:
    name: $CHECKPOINT_BUCKET_NAME
    mode: MOUNT
  # Register Llama asset cards so fairseq2 can download gated models from HF.
  # See https://facebookresearch.github.io/fairseq2/stable/basics/assets.html
  ~/.config/fairseq2/assets/llama_hf.yaml: llama_hf_assets.yaml

setup: |
  # Install system dependencies (libsndfile required by fairseq2n)
  sudo apt-get update && sudo apt-get install -y libsndfile1

  # Create virtual environment
  uv venv --python 3.11 --seed
  source .venv/bin/activate

  # Install PyTorch 2.7.1 with CUDA 12.6 and fairseq2 nightly
  # fairseq2 native (fairseq2n) requires an exact PyTorch+CUDA match;
  # see https://github.com/facebookresearch/fairseq2#variants
  uv pip install torch==2.7.1 torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu126
  uv pip install fairseq2 --pre \
    --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/nightly/pt2.7.1/cu126
  uv pip install huggingface_hub

  # Login to HuggingFace for model access (if token provided)
  if [ -n "$HF_TOKEN" ]; then
    hf auth login --token $HF_TOKEN
  fi

  # Pre-download model weights on every node. fairseq2 only downloads on
  # rank 0 and expects a shared filesystem; we must cache on each node.
  HF_REPO=$(grep -A3 "name: ${MODEL}@" ~/.config/fairseq2/assets/llama_hf.yaml | grep checkpoint | sed 's/.*hg:\/\///' | tr -d '"')
  if [ -n "$HF_REPO" ]; then
    echo "Pre-downloading model $HF_REPO..."
    hf download "$HF_REPO"
  fi

  # Download the GSM8K dataset formatted for fairseq2
  echo "Downloading fairseq2-lm-gsm8k dataset..."
  mkdir -p ~/datasets/facebook/fairseq2-lm-gsm8k

  # Download all dataset files
  hf download facebook/fairseq2-lm-gsm8k \
    --repo-type dataset --local-dir ~/datasets/facebook/fairseq2-lm-gsm8k

  echo "Dataset download completed."

run: |
  set -e
  # Unset CONDA_PREFIX so fairseq2n uses system libsndfile (from apt)
  # instead of searching only in conda's site-packages.
  unset CONDA_PREFIX
  source .venv/bin/activate

  # Extract master address for distributed fine-tuning
  MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  MASTER_PORT=29500

  echo "Node rank: $SKYPILOT_NODE_RANK"
  echo "Master address: $MASTER_ADDR:$MASTER_PORT"
  echo "Total nodes: $SKYPILOT_NUM_NODES"
  echo "GPUs per node: $SKYPILOT_NUM_GPUS_PER_NODE"

  # Use local dir for training output (logs use append, unsupported by MOUNT).
  # Symlink checkpoints/ to the shared bucket mount so all nodes can write shards.
  # NOTE: On Slurm with NFS, you may skip the symlink and use the NFS path directly.
  OUTPUT_DIR=~/output/${MODEL}-multinode
  mkdir -p $OUTPUT_DIR
  ln -sfn /output $OUTPUT_DIR/checkpoints

  # Run distributed fine-tuning with torchrun
  torchrun \
    --nnodes=$SKYPILOT_NUM_NODES \
    --nproc_per_node=$SKYPILOT_NUM_GPUS_PER_NODE \
    --node_rank=$SKYPILOT_NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    -m recipes.lm.sft \
    $OUTPUT_DIR \
    --config \
    model.name=$MODEL \
    tokenizer.name=$MODEL \
    regime.num_steps=$MAX_NUM_STEPS \
    dataset.max_seq_len=$MAX_SEQ_LEN \
    dataset.max_num_tokens=$((MAX_SEQ_LEN * 2)) \
    trainer.mixed_precision.dtype=bfloat16 \
    gang.tensor_parallel_size=$SKYPILOT_NUM_GPUS_PER_NODE \
    common.no_sweep_dir=true

  echo "Distributed fine-tuning completed! Checkpoints saved to: $OUTPUT_DIR"
